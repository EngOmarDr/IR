import numpy as np
import torch
import faiss  # âœ… Ø¯Ø¹Ù… FAISS
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import issparse
from preprocessing import clean_text

def retrieve_top_k_index(query_text, vectorizer, index, doc_ids, corpus, top_k=10):
    query_cleaned = clean_text(query_text)

    # âœ… 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    if hasattr(vectorizer, 'transform'):
        # âœ… TF-IDF
        query_vector = vectorizer.transform([query_cleaned])
        if hasattr(query_vector, "toarray"):
            query_vector = query_vector.toarray()

    elif hasattr(vectorizer, '__getitem__') and hasattr(vectorizer, 'vector_size'):
        # âœ… Word2Vec
        tokens = query_cleaned.split()
        vectors = [vectorizer[word] for word in tokens if word in vectorizer]
        if vectors:
            query_vector = np.mean(vectors, axis=0).reshape(1, -1)
        else:
            query_vector = np.zeros((1, vectorizer.vector_size))

    elif isinstance(vectorizer, tuple) and len(vectorizer) == 2:
        # âœ… BERT
        tokenizer, model = vectorizer
        model.eval()
        with torch.no_grad():
            inputs = tokenizer(query_cleaned, return_tensors='pt', truncation=True, padding=True)
            outputs = model(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
            query_vector = embedding.reshape(1, -1)

    elif isinstance(vectorizer, tuple) and len(vectorizer) == 3:
        # âœ… Hybrid = TF-IDF + BERT â†’ Ù„ÙƒÙ† Ù†Ø¬Ø¹Ù„Ù‡ dense Ø§Ù„Ø¢Ù†
        tfidf_vectorizer, tokenizer, model = vectorizer
        model.eval()

        tfidf_vec = tfidf_vectorizer.transform([query_cleaned])
        if issparse(tfidf_vec):
            tfidf_vec = tfidf_vec.toarray().astype(np.float32)  # â† ğŸ‘ˆ ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ dense

        with torch.no_grad():
            inputs = tokenizer(query_cleaned, return_tensors='pt', truncation=True, padding=True)
            outputs = model(**inputs)
            bert_vec = outputs.last_hidden_state.mean(dim=1).squeeze().numpy().astype(np.float32).reshape(1, -1)

        # ğŸ‘‡ Ù‡Ù†Ø§ Ù†Ø¯Ù…Ø¬ ÙƒÙ€ numpy Ù…Ø¨Ø§Ø´Ø±Ø©
        query_vector = np.hstack([tfidf_vec, bert_vec])  # â† ğŸ‘ˆ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† hstack sparse



    else:
        raise ValueError("Unknown vectorizer type!")

    # âœ… 2. Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS Ø£Ùˆ NearestNeighbors
    if isinstance(index, faiss.Index):
        query_vector = np.asarray(query_vector, dtype=np.float32)
        if query_vector.ndim == 1:  
            query_vector = query_vector.reshape(1, -1)
        distances, indices = index.search(query_vector, top_k)
    else:
        if issparse(query_vector):
            query_vector = query_vector.tocsr()

        distances, indices = index.kneighbors(query_vector, n_neighbors=top_k)

    # âœ… 3. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    results = []
    for dist, idx in zip(distances[0], indices[0]):
        doc_id = doc_ids[idx]
        results.append({
            "doc_id": doc_id,
            "text": corpus[doc_id]["text"],
            "score": 1 - dist  # Ù„Ø£Ù† Ø§Ù„Ù…Ø³Ø§ÙØ© cosine
        })

    return results
