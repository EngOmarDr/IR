import json
import os
import joblib
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
import numpy as np
from transformers import AutoTokenizer, AutoModel
import torch
from scipy.sparse import hstack
from preprocessing import clean_text
from scipy.sparse import issparse


# ----------------------------- ğŸ”¹ Utility ğŸ”¹ -----------------------------
def load_clean_texts(jsonl_path, field='text'):
    texts = []
    ids = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            ids.append(data['_id'])
            texts.append(data[field])
    return ids, texts

def ensure_dir(path):
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)



# âœ… ÙÙŠ Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù (Ø®Ø§Ø±Ø¬ Ø£ÙŠ Ø¯Ø§Ù„Ø©)
def custom_tokenizer(text):
    return clean_text(text).split()

# -------------------------- ğŸ”¹ TF-IDF ğŸ”¹ --------------------------
def build_tfidf(corpus_path, vectorizer_path, matrix_path):
    print("ğŸ” [TF-IDF] ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù†Ø¸ÙØ©...")
    ids, texts = load_clean_texts(corpus_path)

    print("âš™ï¸ [TF-IDF] ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ TF-IDF ...")
    vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=50000)
    tfidf_matrix = vectorizer.fit_transform(texts)

    print("ğŸ’¾ [TF-IDF] Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù€ matrix ...")
    ensure_dir(vectorizer_path)
    joblib.dump(vectorizer, vectorizer_path)
    joblib.dump((ids, tfidf_matrix), matrix_path)

    print("âœ… [TF-IDF] ØªÙ… ØªÙ…Ø«ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ù†Ø¬Ø§Ø­.")

# -------------------------- ğŸ”¹ Word2Vec ğŸ”¹ --------------------------
def build_word2vec(corpus_path, model_path, matrix_path, vectorizer_path):
    print("ğŸ”¤ [W2V] ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù†Ø¸ÙØ© ...")
    ids, texts = load_clean_texts(corpus_path)
    tokenized_texts = [text.split() for text in texts]

    print("âš™ï¸ [W2V] ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Word2Vec ...")
    model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=2, workers=4)

    print("ğŸ“Š [W2V] Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ...")
    doc_embeddings = []
    for tokens in tqdm(tokenized_texts):
        vectors = [model.wv[word] for word in tokens if word in model.wv]
        avg_vec = np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)
        doc_embeddings.append(avg_vec)

    print("ğŸ’¾ [W2V] Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙˆØ§Ù„Ù€ vectorizer...")
    ensure_dir(model_path)
    model.save(model_path)

    ensure_dir(matrix_path)        # <--- ØªØ£ÙƒØ¯ ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª
    joblib.dump((ids, doc_embeddings), matrix_path)

    ensure_dir(vectorizer_path)
    joblib.dump(model.wv, vectorizer_path)

    print("âœ… [W2V] ØªÙ… ØªÙ…Ø«ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ù†Ø¬Ø§Ø­.")



    # -------------------------- ğŸ”¹ BERT ğŸ”¹ --------------------------
def build_bert(corpus_path, model_path, matrix_path, vectorizer_path, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    print("ğŸ¤– [BERT] ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù†Ø¸ÙØ© ...")
    ids, texts = load_clean_texts(corpus_path)

    print(f"ğŸ“¦ [BERT] ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
    model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
    model.eval()

    print("ğŸ“Š [BERT] Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ...")
    doc_embeddings = []

    with torch.no_grad():
        for text in tqdm(texts):
            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
            outputs = model(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
            doc_embeddings.append(embedding)

    print("ğŸ’¾ [BERT] Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ...")
    ensure_dir(matrix_path)
    joblib.dump((ids, doc_embeddings), matrix_path)

    print("ğŸ’¾ [BERT] Ø­ÙØ¸ vectorizer (tokenizer + model) ...")
    ensure_dir(vectorizer_path)
    joblib.dump((tokenizer, model), vectorizer_path)  # âœ… Ù‡Ù†Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯

    print("âœ… [BERT] ØªÙ… ØªÙ…Ø«ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ù†Ø¬Ø§Ø­.")



# -------------------------- ğŸ”¹ Hybrid ğŸ”¹ --------------------------

def build_hybrid(tfidf_matrix_path, bert_vectors_path, output_path):
    print("ğŸ§¬ [Hybrid] ØªØ­Ù…ÙŠÙ„ ØªÙ…Ø«ÙŠÙ„Ø§Øª TF-IDF Ùˆ BERT ...")
    tfidf_ids, tfidf_matrix = joblib.load(tfidf_matrix_path)
    bert_ids, bert_vectors = joblib.load(bert_vectors_path)

    assert tfidf_ids == bert_ids, "âŒ IDs mismatch between TF-IDF and BERT!"

    print("ğŸ”— [Hybrid] Ø¯Ù…Ø¬ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ÙŠÙ† ...")

    # âœ… ØªØ£ÙƒØ¯ Ø£Ù† BERT Ù‡Ùˆ dense NumPy matrix
    bert_matrix = np.vstack(bert_vectors).astype(np.float32)

    # âœ… Ø¯Ù…Ø¬ sparse + dense Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… hstack
    from scipy.sparse import hstack, csr_matrix
    bert_sparse = csr_matrix(bert_matrix)  # ØªØ­ÙˆÙŠÙ„ bert Ø¥Ù„Ù‰ sparse Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø¯Ù…Ø¬

    from scipy.sparse import hstack
    hybrid_sparse = hstack([tfidf_matrix, bert_sparse])
     # â›³ Ø­ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ dense Ù…Ø³Ø¨Ù‚Ù‹Ø§


    print("ğŸ’¾ [Hybrid] Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© ...")
    ensure_dir(output_path)
    joblib.dump((tfidf_ids, hybrid_sparse), output_path)

    print("âœ… [Hybrid] ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­.")





# -------------------------- ğŸ”¹ MAIN ğŸ”¹ --------------------------
if __name__ == "__main__":
    dataset = "quora"  # Ø¨Ø¯Ù„Ù‹Ø§ Ù…Ù† "quora"
    data_dir = f"data/{dataset}"
    vector_store = "vector_stores"

    corpus_path = os.path.join(data_dir, "cleaned_corpus.jsonl")

    # âœ… 1. ØªÙ…Ø«ÙŠÙ„ TF-IDF
    build_tfidf(
        corpus_path=corpus_path,
        vectorizer_path=os.path.join(vector_store, f"{dataset}_tfidf_vectorizer.joblib"),
        matrix_path=os.path.join(vector_store, f"{dataset}_tfidf_matrix.joblib")
    )

    # âœ… 2. ØªÙ…Ø«ÙŠÙ„ Word2Vec
    build_word2vec(
    corpus_path=corpus_path,
    model_path=os.path.join(vector_store, f"{dataset}_word2vec.model"),
    matrix_path=os.path.join(vector_store, f"{dataset}_word2vec_vectors.joblib"),
    vectorizer_path=os.path.join(vector_store, f"{dataset}_word2vec_vectorizer.joblib")  # âœ… Ø§Ù„Ø¬Ø¯ÙŠØ¯
    )

        # âœ… 3. ØªÙ…Ø«ÙŠÙ„ BERT
    build_bert(
        corpus_path=corpus_path,
        model_path=None,  # Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†ÙØµÙ„Ù‹Ø§
        matrix_path=os.path.join(vector_store, f"{dataset}_bert_vectors.joblib"),
        vectorizer_path=os.path.join(vector_store, f"{dataset}_bert_vectorizer.joblib")  # âœ… Ø§Ù„Ø¬Ø¯ÙŠØ¯
    )

    # âœ… 4. Hybrid = TF-IDF + BERT
    build_hybrid(
        tfidf_matrix_path=os.path.join(vector_store, f"{dataset}_tfidf_matrix.joblib"),
        bert_vectors_path=os.path.join(vector_store, f"{dataset}_bert_vectors.joblib"),
        output_path=os.path.join(vector_store, f"{dataset}_hybrid_vectors.joblib")
    )



